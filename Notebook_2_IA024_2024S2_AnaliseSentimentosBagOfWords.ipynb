{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook 2 IA-024-2024S2 FEEC-UNICAMP - Lucas Couto Lima RA: 220696"
      ],
      "metadata": {
        "id": "Oe-Z6EpFRypT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalação e importação de pacotes"
      ],
      "metadata": {
        "id": "XRnOFwqk23W4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets portalocker>=2.0.0 -q"
      ],
      "metadata": {
        "id": "HA5BWLDCKmw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from collections import Counter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Adicionados\n",
        "import time\n",
        "import string"
      ],
      "metadata": {
        "id": "VorDvF62iyXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I - Vocabulário e Tokenização"
      ],
      "metadata": {
        "id": "A5ovJE02CwKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Converter para minúsculas\n",
        "    text = text.lower()\n",
        "    # Remover pontuações\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "train_dataset = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
        "\n",
        "vocab_size = 20000\n",
        "\n",
        "counter = Counter()\n",
        "for sample in train_dataset: #\n",
        "    preprocessed_text = preprocess_text(sample[\"text\"])  # Pré-processar o texto\n",
        "    counter.update(preprocessed_text.split())  # Atualizar o contador com tokens\n",
        "\n",
        "# create a vocabulary of the 20000 most frequent tokens\n",
        "most_frequent_words = sorted(counter, key=counter.get, reverse=True)[:vocab_size]\n",
        "vocab = {word: i for i, word in enumerate(most_frequent_words, 1)}\n",
        "vocab_size = len(vocab)"
      ],
      "metadata": {
        "id": "mqzUqy3diz0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence, vocab):\n",
        "    sentence = preprocess_text(sentence) # Modificação\n",
        "    return [vocab.get(word, 0) for word in sentence.split()] # 0 for OOV"
      ],
      "metadata": {
        "id": "0rZn-m1Mi110"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II - Dataset"
      ],
      "metadata": {
        "id": "5iV4bF8cDAj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import one_hot\n",
        "# Dataset Class with One-hot Encoding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, split, vocab):\n",
        "        self.data = load_dataset(\"stanfordnlp/imdb\", split=split)\n",
        "        self.vocab = vocab\n",
        "        # Pré-processar os dados para one-hot encoding\n",
        "        self.encoded_data = []\n",
        "        for sample in self.data:\n",
        "            target = sample[\"label\"]\n",
        "            line = sample[\"text\"]\n",
        "            target = 1 if target == 1 else 0\n",
        "            # one-hot encoding\n",
        "            X = torch.zeros(len(self.vocab) + 1)\n",
        "            for word in encode_sentence(line, self.vocab):\n",
        "                X[word] = 1\n",
        "            self.encoded_data.append((X, torch.tensor(target)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encoded_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.encoded_data[idx]\n",
        "\n",
        "# Load Data with One-hot Encoding\n",
        "train_data = IMDBDataset('train', vocab)\n",
        "test_data = IMDBDataset('test', vocab)"
      ],
      "metadata": {
        "id": "VDUyZoTPi262"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III - Data Loader"
      ],
      "metadata": {
        "id": "d7RMPSvMDL5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "# define dataloaders\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(test_data,  batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Y7tcZv2YDIog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IV - Modelo"
      ],
      "metadata": {
        "id": "MwPeJ7h8DahT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotMLP(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(OneHotMLP, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(vocab_size+1, 200)\n",
        "        self.fc2 = nn.Linear(200, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        o = self.fc1(x.float())\n",
        "        o = self.relu(o)\n",
        "        return self.fc2(o)\n",
        "\n",
        "# Model instantiation\n",
        "model = OneHotMLP(vocab_size)"
      ],
      "metadata": {
        "id": "6QuDhWvji7lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## V - Laço de Treinamento - Otimização da função de Perda pelo Gradiente descendente"
      ],
      "metadata": {
        "id": "iVAhdFGXDepU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se há uma GPU disponível e define o dispositivo para GPU se possível,\n",
        "# caso contrário, usa a CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device.type == 'cuda':\n",
        "    print('GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
        "else:\n",
        "    print('using CPU')"
      ],
      "metadata": {
        "id": "RaH1Uv3yHih5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8edcdd0-ccdf-4952-a4c2-8776228a1c71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Função para calcular loss e acurácia\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits.squeeze(), targets.float())\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            predicted = (torch.sigmoid(logits) >= 0.5).float()\n",
        "            correct_predictions += (predicted.squeeze() == targets).sum().item()\n",
        "            total_samples += targets.size(0)\n",
        "\n",
        "    average_loss = total_loss / len(data_loader.dataset)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return average_loss, accuracy\n",
        "\n",
        "# Avaliação inicial\n",
        "initial_train_loss, _ = evaluate(model, train_loader)\n",
        "initial_val_loss, initial_val_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "print(f'Initial Train Loss: {initial_train_loss:.4f}')\n",
        "print(f'Initial Validation Loss: {initial_val_loss:.4f}, Validation Accuracy: {initial_val_accuracy:.4f}')\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    total_epoch_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        # Forward pass\n",
        "        logits = model(inputs)\n",
        "        loss = criterion(logits.squeeze(), targets.float())\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumula a Loss do batch\n",
        "        total_epoch_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Calcula a Loss média da época\n",
        "    average_epoch_loss = total_epoch_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Avaliação no conjunto de validação\n",
        "    val_loss, val_accuracy = evaluate(model, test_loader)\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Train Loss: {average_epoch_loss:.4f}, '\n",
        "          f'Validation Loss: {val_loss:.4f}, '\n",
        "          f'Validation Accuracy: {val_accuracy:.4f}, '\n",
        "          f'Elapsed Time: {epoch_duration:.2f} sec')"
      ],
      "metadata": {
        "id": "Nh_pe8rni93_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f6d6b3-8684-444e-c112-c4e6f11c4a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Train Loss: 0.6930\n",
            "Initial Validation Loss: 0.6930, Validation Accuracy: 0.5026\n",
            "Epoch [1/5], Train Loss: 0.6924, Validation Loss: 0.6918, Validation Accuracy: 0.5180, Elapsed Time: 14.63 sec\n",
            "Epoch [2/5], Train Loss: 0.6911, Validation Loss: 0.6906, Validation Accuracy: 0.5355, Elapsed Time: 14.66 sec\n",
            "Epoch [3/5], Train Loss: 0.6898, Validation Loss: 0.6893, Validation Accuracy: 0.5610, Elapsed Time: 15.12 sec\n",
            "Epoch [4/5], Train Loss: 0.6884, Validation Loss: 0.6878, Validation Accuracy: 0.5907, Elapsed Time: 15.04 sec\n",
            "Epoch [5/5], Train Loss: 0.6868, Validation Loss: 0.6862, Validation Accuracy: 0.6248, Elapsed Time: 14.63 sec\n"
          ]
        }
      ]
    }
  ]
}